---
title: "Twitter Data Analysis"
subtitle: "infoXpand Workshop - Göttingen"
author: "André Calero Valdez"
date: "`r Sys.Date()`"
output: 
  xaringan::moon_reader:
    css: ["default", "default-fonts", "imis.css"]  
    lib_dir: libs
    self_contained: true
    nature:
      ratio: '16:9'
      highlightStyle: github
      countIncrementalSlides: false
---

```{r setup, include=FALSE}
set.seed(123)
knitr::opts_chunk$set(
  echo = FALSE, 
  fig.retina = 2,
  fig.width = 8,
  fig.height = 6)

library(data.table)
library(tidyverse)
library(stringr)
library(zoo)
library(tidytext)
library(stopwords)
library(kableExtra)

source(here::here("R", "utils.R"))
source(here::here("R", "read_data.R"))

inc_file_name <- here::here("data", "COVID-19-Faelle_7-Tage-Inzidenz_Deutschland.csv")
raw_csv_datafile <- here::here("data", "merged.csv")
#db_file_name <- here::here("data", "my-db.sqlite")

incidences <- load_incidences(inc_file_name)
#all_data <- read_twitter_data(raw_csv_datafile)
all_data <- read_rds(here::here("data", "all_data.rds"))
all_data <- read_rds(here::here("data", "sample_data.rds"))

# generate a stopwords list
custom_stop_words <- c("rt", "https", "t.co", "mehr", "schon", "mal", "ja", "seit", "wurde", "wer", "warum",
                       "1", "2", "3", "4", "5", "6", "7", "8", "9", "0", "wäre", "wurden", "würde", "würden",
                       "wegen", "gibt", "ab", "d", "2020", "ganz", "viele", "beim", "amp", "bitte", "immer", "nie",
                       "lassen", "kommt", "müssen", "eigentlich", "wohl", "endlich", "hätte", "herr", "erst", "u",
                       "fast", "gar", "dafür", "nein", "klar", "tun")

stop_words <- tibble(word = c(stopwords::data_stopwords_nltk$de, custom_stop_words))

len <- all_data |> nrow() |> format(big.mark=",")

```


## Analysis of Twitter data

* Collection of Twitter Streaming API data
  * All tweets from Germany, related to covid
* Collected using google cloud platform and research API access

### Overview

* `r len ` tweets from Aug-2022 until Jul-2023
  * Data error in first month (AND not OR on features)
  * Data gap in March 2023 (API change)
  * July 2023 Twitter changed API access (expensive) 


---


## Variables

```{r}
names(all_data) |> as_tibble() |> knitr::kable()
```

---

<small>
```{r}
all_data |> 
  sample_n(10) |> 
  knitr::kable() |> 
  kable_styling("striped", full_width = F) |> 
  kable_styling(font_size = 9)
```
</small>


---

## Descriptive data (language)

Mostly, german tweets

```{r}
all_data |> 
  dplyr::select(lang) |> 
  ggplot() +
  aes(x = lang |> 
        fct_infreq() |> 
        fct_lump(n = 10) |> 
        fct_rev() 
      ) +
  geom_bar() +
  coord_flip() +
  labs(title = "Language distribution", x = "language code", y = "frequency")

```



---


## Sample of our twitter data over time

```{r tweets-over-time}
all_data %>%
  dplyr::select(created_at) %>% 
  ggplot() + 
  aes(x = created_at) +
  geom_histogram(bins = 50) +
  scale_y_continuous(labels = scales::comma) +
  labs(title = "Tweets over time", x = "Date", y = "Number of Tweets")
```

---

## Covid-19 Incidence data (RKI)

```{r plot_incidences}
incidences %>% 
  ggplot() +
  aes(x = Meldedatum, y = `Faelle_7-Tage`, group = 1) + 
  geom_line() +
  scale_y_continuous(labels = scales::comma) +
  scale_x_date(date_labels = "%b %y") +
  labs(y = "7-Tage-Inzidenz", x = "Meldedatum")
```

---

## Relevant Time frame 
Same time as tweet availability

```{r plot_incidences_relevant, echo=FALSE}
releveant_incidences <- incidences %>% 
  filter(Meldedatum > ymd("2022-08-05")) %>% 
  filter(Meldedatum < ymd("2023-07-01")) %>%
  # altersgruppen zusammenführen
  filter(Altersgruppe == "00+") %>% 
  noop() 
  
releveant_incidences %>% 
  ggplot() +
  aes(x = Meldedatum, y = `Faelle_7-Tage`, group = 1) + 
  geom_line() +
  scale_y_continuous(labels = scales::comma) +
  scale_x_date(date_labels = "%b %y") +
  labs(y = "7-Tage-Inzidenz", x = "Meldedatum")
```

---

## Example: corona token
How are "corona" as a token and incidence connected?

```{r avg_tweets_de}
filter_word <- "corona"

all_data %>% 
  dplyr::filter(lang == "de") %>% 
  mutate(text = str_to_lower(text)) %>% 
  filter(str_detect(text, fixed(filter_word))) %>%
  select(created_at) %>% 
  mutate(created_at_day = as_date(floor_date(created_at, "day"))) %>%
  group_by(created_at_day) %>%
  summarize(tweet_count = n()) %>% 
  mutate(tweet_count = rollmean(tweet_count, 7, fill = NA, align = "right")) %>%
  noop() -> avg_tweets_de_filtered
```


```{r plot_avg_tweets_de}
# plot tweet weekly averages ----
avg_tweets_de_filtered %>% 
  ggplot() + 
    aes(x = created_at_day, y = tweet_count) + 
    geom_line() +
  labs(title = filter_word)

```


---

## Token Data

```{r filter-to-lower-data}
lower_de_data <- all_data %>% 
  filter(lang == "de") %>% 
  mutate(text = str_to_lower(text)) %>% 
  mutate(created_at_day = as_date(floor_date(created_at, "day"))) %>% 
  noop()
```

```{r tokenize-data}
lower_de_data %>% 
  #sample_n(100000) %>% 
  select(created_at_day, text) %>% 
  unnest_tokens(word, text) %>% 
  group_by(created_at_day, word) %>%
  summarize(word_count = n()) %>% 
  ungroup() %>% 
  arrange(desc(word_count)) %>% 
  anti_join(stop_words, by = c("word" = "word")) %>%
  noop() -> tokenized_data
```

---


```{r top_words}
# test for all words
tokenized_data %>% 
  group_by(word) %>% 
  summarize(tweet_count = sum(word_count) ) %>% 
  arrange(desc(tweet_count)) -> top_words

top_words %>%
  head(100) %>%
  ggplot() +
  aes(x = reorder(word, tweet_count), y = tweet_count) +
  geom_col() +
  coord_flip()
```

---

## Example comparison

```{r generate-rolling-average-example}
idx <- 4
roll_avg <- tokenized_data %>% 
  filter(word == top_words$word[idx]) %>% 
  mutate(word_count = rollmean(word_count, 7, fill = NA, align = "right")) %>% 
  noop()

tmax <- roll_avg %>% pull(word_count) %>% max(na.rm = TRUE)

imax <- releveant_incidences %>% 
  filter(Altersgruppe == "00+") %>% 
  select(datum = Meldedatum, cases = `Faelle_7-Tage`) %>% 
  pull(cases) %>% max(na.rm = TRUE)

max_ratio <- imax/tmax

```

```{r plot-rolling-average-example}
releveant_incidences %>% 
  select(datum = Meldedatum, cases = `Faelle_7-Tage`) %>%
  left_join(roll_avg, by = c(datum = "created_at_day")) %>% 
  select(-word) %>% 
  ggplot() +
  aes(x = datum) +
  geom_line(aes(y = cases, color = "red", name = "cases")) +
  geom_line(aes(y = word_count*max_ratio, color = "blue")) +
  scale_y_continuous(
  
    # Features of the first axis
    name = "cases",
    
    # Add a second axis and specify its features
    sec.axis = sec_axis(~./max_ratio, name="term frequency")
  ) +
  labs(title = paste("Rolling 7-day average for term:",top_words$word[idx]),
       x = "date", y = "term frequency") 
```


---



```{r generate-merged-rolling-averages}

result_data <- releveant_incidences 

for(i in 1:100){
  
  idx <- i
  col_name <- top_words$word[idx]
  
  roll_avg <- tokenized_data %>% 
    dplyr::filter(word == top_words$word[idx]) %>% 
    dplyr::arrange(created_at_day) %>%
    complete(created_at_day = seq(min(created_at_day),max(created_at_day),1), fill = list(word = top_words$word[idx], word_count = 0)) |> 
    dplyr::mutate({{col_name}} := rollmean(word_count, 7, fill = c(0,0,0), align = "right")) %>%
    dplyr::select(created_at_day, {{col_name}}) %>%
    noop()
  roll_avg
  
result_data <- result_data %>% 
  left_join(roll_avg, by = c(Meldedatum = "created_at_day")) 
}
result_data
```


## Generate Time Series Data

```{r remove_superfluous_data}
ts_data <- result_data %>% 
  dplyr::select(-Altersgruppe, -Bevoelkerung, -Faelle_gesamt, -Faelle_neu, -Meldedatum, -`Inzidenz_7-Tage`) |> 
  #dplyr::select(1:10) |> 
  ts()


```


---

```{r load_timeseries_libs}
library(forecast)
library(vars)
library(coefplot)
#ts_data  |> plot()
```

---

## Stationary series?
If not stationary make stationary
```{r make-stationary}
ndiffs(ts_data)

diff_ts <- diff(ts_data, differences = 1) |> na.omit()
diff_ts |> plot(plot.type = "single")


```

---

## Multivariate Autoregressive Model


$$
Y_t = c + A_1Y_{t-1} + A_2Y_{t-2} + \cdots + A_pY_{t-p} + u_t
$$

Where:

- $Y_t$ is a $k \times 1$ vector of endogenous variables at time $t$,
- $c$ is a $k \times 1$ vector of constants (intercept),
- $A_1, A_2, \ldots, A_p$ are $k \times k$ matrices of coefficients to be estimated,
- $u_t$ is a $k \times 1$ vector of error terms that are assumed to be gaussian noise,
- $p$ is the number of lagged observations included in the model (the order of the VAR),
- $t$ represents the time period.


---

```{r multivar, fig.width=10, fig.height=20}

ts_var <- VAR(diff_ts, lag.max = 7) 

ts_var$varresult$Faelle_7.Tage |> coef() |> head()
```


---

## Coefficient plot
```{r multivar_coef, fig.width=10, fig.height=20}
ts_var$p

coefplot(ts_var$varresult$Faelle_7.Tage, sort = "magnitude")

```


---

## Identify significant coefficients

```{r multivar_significant}

sum_var <-  ts_var$varresult$Faelle_7.Tage |> summary()

svt <- sum_var$coefficients |> 
  as.data.frame() |> 
  rownames_to_column() |> 
  as_tibble() |> 
  mutate(p_value = `Pr(>|t|)`)


svt |> filter(p_value < 0.05) |> knitr::kable()

```


